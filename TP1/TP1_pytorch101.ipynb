{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion a Pytorch\n",
    "\n",
    "Pytorch es una libreria que permite trabajar con vectores y matrices de muchas dimensiones.\n",
    "\n",
    "En ese sentido es muy parecido a Numpy. En numpy a los vectores son llamados arrays. En pytorch se los llama tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.,2.],[3.,4.]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El shape del tensor es una lista del tamano de cada dimension. Si trabajamos con matrices (como comumnmente se llama a los vectores 2D), cada \"fila\" tiene la misma longitud: no hay una fila mas corta que otra.\n",
    "\n",
    "Lo mismo vale para mas dimensiones. No podemos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "torch.tensor([[1.,2.],[3.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que introduce de nuevo pytorch sobre numpy\n",
    "Esencialmente dos cosas:\n",
    "- Paralelizacion en GPU\n",
    "- Calculo automatico de derivadas (gradiente)\n",
    "\n",
    "Estas funciones son muy deseables cuando trabajamos con redes neuronales. Aunque tambien hay librerias de, por ejemplo, algebra lineal que han aprovechado esto para ser escritas sobre pytorch y funcionar eficientemente.\n",
    "\n",
    "Pytorch tambien provee muchas de las funcionalidades necesarias para definir una red y entrenarla.\n",
    "\n",
    "Con la siguiente funcion podemos ver si tenemos una GPU disponible en nuestro sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar la velocidad entre ejecutar algo en CPU o usando GPU (si tenemos).\n",
    "\n",
    "Pytorch requiere mover explicitamente los tensores a la GPU, se puede hacer facilmente mediante el metodo .cuda()\n",
    "\n",
    "(Si se operan vectores que estan en la GPU con vectores que estan en la GPU causara un error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(1000,1000)\n",
    "B = torch.rand(1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 ms ± 41.2 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    A = A.cuda()\n",
    "    B = B.cuda()\n",
    "    print(\"CUDA available\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53 ms ± 17 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenés iOS (Mac) podés ver como acelerar Pytorch con el backedn MPS (solo disponible para AMD o Sillicon Chip, NO Intel).\n",
    "[Ver la documentación](https://developer.apple.com/metal/pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"MPS available\")\n",
    "    A = A.to(mps_device)\n",
    "    B = B.to(mps_device)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721 μs ± 4.06 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1: \"Usando una GPU\" (opcional)\n",
    "\n",
    "Ejecutar el codigo anterior en una GPU. Para ello se pueden utilizar las GPU de papparspace u otro proveedor cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Autograd\n",
    "\n",
    "Pytorch provee la funcionalidad \"recordar\" como cada vector fue calculado con el fin de computar el gradiente.\n",
    "\n",
    "Podemos definir vectores con ciertos valores y crear otros como resultado de operar los primeros y pytorch recordara el grafo de las computaciones.\n",
    "\n",
    "Supongamos que tenemos los valores:\n",
    "\n",
    "$ a = 1 $\n",
    "\n",
    "$ b = 2 $\n",
    "\n",
    "$ c = 0 $\n",
    "\n",
    "Y definimos $m$, $n$ (capas intermedias), $p$ (una prediccion) y $l$ (un costo) como:\n",
    "\n",
    "$ m = a + b $\n",
    "\n",
    "$ n = max(b,c) $\n",
    "\n",
    "$ p = m \\times n $\n",
    "\n",
    "$ l = p^2 $\n",
    "\n",
    "Pytorch calculara los valores intermedios dado el valor de las hojas:\n",
    "\n",
    "$ m = 3 $\n",
    "\n",
    "$ n = 2 $\n",
    "\n",
    "$ p = 6 $\n",
    "\n",
    "$ l = 36 $\n",
    "\n",
    "Y a la vez construira el siguiente grafo de computaciones:\n",
    "\n",
    "![title](graph.png)\n",
    "\n",
    "Si $a$,$b$,$c$ son nuestros parametros y queremos minimizar el costo $l$. Querremos calcular: $\\Large\\frac{\\partial l}{\\partial a}$, $\\Large\\frac{\\partial l}{\\partial b}$, $\\Large\\frac{\\partial l}{\\partial c}$\n",
    "\n",
    "A primera vista no es fácil calcular dichas derivadas. En general, usando la definición de los valores, podemos calcular la derivada de un valor en función de los valores que dependen de forma directa de él. Haciendo referencia al grafo de las computaciones (la imagen de arriba), podemos calcular el valor de $\\large\\frac{\\partial y}{\\partial x}$ si existe una arista que $x \\rightarrow y$. Así, podemos calcular las siguientes derivadas, ya que cada variable depende de forma directa una de la otra:\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial p} = 2 \\times p = 12$\n",
    "\n",
    "$\\large\\frac{\\partial p}{\\partial m} = n = 2$\n",
    "\n",
    "$\\large\\frac{\\partial p}{\\partial n} = m = 3$\n",
    "\n",
    "$\\large\\frac{\\partial m}{\\partial a} = 1$\n",
    "\n",
    "$\\large \\frac{\\partial m}{\\partial b} = 1$\n",
    "\n",
    "$\\large \\frac{\\partial n}{\\partial b} = 1$\n",
    "\n",
    "$\\large \\frac{\\partial n}{\\partial c} = 0$\n",
    "\n",
    "Pero como hacemos para calcular las derivadas $\\Large\\frac{\\partial l}{\\partial a}$, $\\Large\\frac{\\partial l}{\\partial b}$, $\\Large\\frac{\\partial l}{\\partial c}$ que son las que necesitamos para actualizar el gradiente? \n",
    "\n",
    "Centrémosnos por un momento en el caso de $\\Large\\frac{\\partial l}{\\partial a}$. Si bien no tenemos una fórmula que relacione de manera directa $a$ con $l$, el valor de $l$ depende del valor de $a$: esto es debido a que el valor de $l$ depende de $p$, que a su vez depende de $m$, que por último depende de $a$.\n",
    "\n",
    "Apliquemos la [regla de la cadena](https://en.wikipedia.org/wiki/Chain_rule) entre $p$ (que es una función de $a$), $l$ (que es función de $p$) y $a$:\n",
    "\n",
    "$\\Large\\frac{\\partial l}{\\partial a} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial a}$\n",
    "\n",
    "Mirando el término de la derecha:\n",
    "- $\\Large \\frac{\\partial l}{\\partial p}$ lo conocemos y vale $12$.\n",
    "- $\\Large \\frac{\\partial p}{\\partial a}$ no lo hemos calculado aún, pero nos hemos \"acercado\" a $a$ en el grafo de las computaciones. Todo indica que si aplicamos una vez más la regla de la cadena terminaremos llegando a $a$. Apliquemos pues, la regla de la cadena sobre $m$, $p$ y $a$:\n",
    "\n",
    "$\\Large\\frac{\\partial p}{\\partial a} = \\frac{\\partial p}{\\partial m} \\times \\frac{\\partial m}{\\partial a}$\n",
    "\n",
    "Ahora, mirando el término de la derecha, tenemos todos los valores: $\\Large \\frac{\\partial p}{\\partial m}$ vale 2 y $\\Large \\frac{\\partial m}{\\partial a}$ vale 1.\n",
    "\n",
    "Ya estamos en condiciones pues de calcular $\\Large\\frac{\\partial l}{\\partial a}$:\n",
    "\n",
    "$\\Large\\frac{\\partial l}{\\partial a} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial a} = 12 \\times \\frac{\\partial p}{\\partial m} \\times \\frac{\\partial m}{\\partial a} = 12 \\times 2 \\times 1 = 24$\n",
    "\n",
    "Ordenando nuestro razonamiento, lo que estamos haciendo es calcular los gradientes respecto de las variables que están más cerca del resultado:\n",
    "\n",
    "$\\Large\\frac{\\partial l}{\\partial m} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial m} = 12 \\times 2 = 24$\n",
    "\n",
    "\n",
    "\n",
    "$\\Large\\frac{\\partial l}{\\partial n} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial n} = 12 \\times 3 = 36$\n",
    "\n",
    "Y luego utilizar dichos gradientes para calcular los de las capas anteriores:\n",
    "\n",
    "$\\Large\\frac{\\partial l}{\\partial a} = \\frac{\\partial l}{\\partial m} \\times \\frac{\\partial m}{\\partial a} = 24 \\times 1 = 24$\n",
    "\n",
    "\n",
    "Podemos hacer lo mismo para calcular $\\Large\\frac{\\partial l}{\\partial c}$.\n",
    "\n",
    "\n",
    "Para el caso de $b$ usamos la [regla de la cadena multivariada](https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/multivariable-calculus/multi-variable-chain-rule/) (es muy parecida a la regla del producto, de hecho la regla del producto es un caso particular de la regla de la cadena multivariada). Esta aplica porque $b$ es usada en varios valores ($m$ y $n$) de los cuales depende en última instancia el valor que queremos diferenciar $l$.\n",
    "\n",
    "$\\Large\\frac{\\partial l}{\\partial b} = \\frac{\\partial l}{\\partial m} \\times \\frac{\\partial m}{\\partial b} + \\frac{\\partial l}{\\partial n} \\times \\frac{\\partial n}{\\partial b} = 24+36 = 60$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a calcular los gradientes usando [Pytorch Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) (diferenciación automática).\n",
    "\n",
    "Primero definimos los valores hoja (de los que depende el resto). Serían nuestro parámetros (respecto de los cuales vamos a derivar después). Es importante el argumento requires_grad para que Pytorch sepa que tiene que calcular el grafo de las computaciones que le apliquemos a dichas variables, porque vamos a querer el gradiente respecto a dichas variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.], requires_grad=True),\n",
       " tensor([2.], requires_grad=True),\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.],requires_grad=True)\n",
    "b = torch.tensor([2.],requires_grad=True)\n",
    "c = torch.tensor([0.],requires_grad=True)\n",
    "a,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos definir otros valores en relación a los anteriores. En grad_fn lo que vemos es que Pytorch está llevando un historial de como dichos valores fueron contruídos. Esto es el grafo de las computaciones que luego le servirá para calcular el gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.], grad_fn=<AddBackward0>),\n",
       " tensor([2.], grad_fn=<MaximumBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = a+b\n",
    "n = torch.max(a,b)\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.], grad_fn=<MulBackward0>), tensor([36.], grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = m*n\n",
    "l = p**2\n",
    "p, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular el gradiente de $l$ basta con llamar l.backwards()\n",
    "\n",
    "Generalmente sólo usamos los gradientes respecto a los parámetros (las hojas). Si bien Pytorch calcula los gradientes respecto a los nodos intermedios (es un resultado parcial que luego ayudará para calcular los gradientes en las hojas), los borra automáticamente luego de usarlos para ahorrar memoria. Esto se puede evitar con el método .retain_grad(). En este caso le pediremos a Pytorch que no los borre ya que los queremos mostrar para corroborar que nos dio el mismo resultado que el que hallamos de manera analítica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m.retain_grad()\n",
    "n.retain_grad()\n",
    "p.retain_grad()\n",
    "l.retain_grad()\n",
    "\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último mostramos los gradientes de $l$ respecto a cada una de las variables. Podemos corroborar que los valores son los mismo que los hallados de manera analítica arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([12.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.grad, p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24.]), tensor([36.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.grad, n.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24.]), tensor([60.]), None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad, c.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación del gradiente\n",
    "\n",
    "Recordar que el gradiente $\\Large \\frac{\\partial l}{\\partial a}$ indica cuánto cambia $l$ ante pequeños cambios de $a$:\n",
    "\n",
    "$\\Large \\frac{\\partial l}{\\partial a} = lim_{\\Delta a \\rightarrow 0} \\frac{\\Delta l}{\\Delta a}$\n",
    "\n",
    "Corroboraremos esto empíricamente, calculando $\\Delta l$ para variaciones $\\Delta a$, $\\Delta b$, $\\Delta c$ muy pequeñas.\n",
    "\n",
    "Para ellos haremos una función que nos calcule $l$ dado $a$, $b$ y $c$c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l(a,b,c):\n",
    "    m = a+b\n",
    "    n = torch.max(a,b)\n",
    "    p = m*n\n",
    "    l = p**2\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeamos que el resultado es efectivamente 36:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov = calculate_l(a, b, c)\n",
    "ov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego calcularemos $l$ introduciendo un pequeño cambio en $a$. Observaremos como resulta el ratio de cambio $\\Large \\frac{\\Delta l}{\\Delta a}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value: 36.02400207519531\n",
      "Change value: 0.0240020751953125\n",
      "Change ration: 24.002073287963867\n"
     ]
    }
   ],
   "source": [
    "small_change = 0.001\n",
    "nv = calculate_l(a + small_change, b, c)\n",
    "print(f'New value: {(nv).item()}')\n",
    "print(f'Change value: {(nv - ov).item()}')\n",
    "print(f'Change ration: {((nv - ov) / small_change).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente el ratio de variación es 24, como la derivada $\\Large \\frac{\\partial l}{\\partial a}$.\n",
    "Haremos lo mismo para $b$ y para $c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value: 36.06003189086914\n",
      "Change value: 0.060031890869140625\n",
      "Change ration: 60.03188705444336\n"
     ]
    }
   ],
   "source": [
    "nv = calculate_l(a , b + small_change, c)\n",
    "nv, (nv - ov) / small_change\n",
    "print(f'New value: {(nv).item()}')\n",
    "print(f'Change value: {(nv - ov).item()}')\n",
    "print(f'Change ration: {((nv - ov) / small_change).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value: 36.0\n",
      "Change value: 0.0\n",
      "Change ration: 0.0\n"
     ]
    }
   ],
   "source": [
    "nv = calculate_l(a , b, c + small_change)\n",
    "nv, (nv - ov) / small_change\n",
    "print(f'New value: {(nv).item()}')\n",
    "print(f'Change value: {(nv - ov).item()}')\n",
    "print(f'Change ration: {((nv - ov) / small_change).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2: \"Calculo del gradiente\" (opcional)\n",
    "\n",
    "Dada las siguientes definiciones:\n",
    "\n",
    "$a = 2$\n",
    "\n",
    "$b = 3$\n",
    "\n",
    "$c = -1$\n",
    "\n",
    "$d = 7$\n",
    "\n",
    "$m = 3 \\times a + 4 \\times b - 2 \\times c + 10 \\times d$\n",
    "\n",
    "$n = 7 \\times a + b + \\times c + 4 \\times d$\n",
    "\n",
    "$p = max(m,0)$\n",
    "\n",
    "$q = max(n,0)$\n",
    "\n",
    "$z = p - q$\n",
    "\n",
    "$ pred = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "$loss = log(pred)$\n",
    "\n",
    "Calcular:\n",
    "\n",
    "1. El grafo de las computaciones (un diagrama de flechas como el de arriba donde se vea la relación de dependencia entre las variables).\n",
    "2. Las derivadas $\\large \\frac{\\partial loss}{\\partial a}$, $\\large \\frac{\\partial loss}{\\partial b}$, $\\large \\frac{\\partial loss}{\\partial c}$, $\\large \\frac{\\partial loss}{\\partial d}$ manualmente.\n",
    "3. Las derivadas $\\large \\frac{\\partial loss}{\\partial a}$, $\\large \\frac{\\partial loss}{\\partial b}$, $\\large \\frac{\\partial loss}{\\partial c}$, $\\large \\frac{\\partial loss}{\\partial d}$ utilizando Pytorch Autograd (backwards).\n",
    "4. Una aproximación de las derivadas $\\large \\frac{\\partial loss}{\\partial a}$, $\\large \\frac{\\partial loss}{\\partial b}$, $\\large \\frac{\\partial loss}{\\partial c}$, $\\large \\frac{\\partial loss}{\\partial d}$ mediante los ratios $\\large \\frac{\\Delta loss}{\\Delta a}$, $\\large \\frac{\\Delta loss}{\\Delta b}$, $\\large \\frac{\\Delta loss}{\\Delta c}$, $\\large \\frac{\\Delta loss}{\\Delta d}$ de cambio utilizando pequeñas variaciones $\\Delta a$, $\\Delta b$, $\\Delta c$, $\\Delta d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando descenso por gradiente para problemas de optimización\n",
    "\n",
    "El método de descenso por gradiente permite optimizar (maximizar o minimizar) cualquier función $f(w)$. Consiste comenzar en cualquier $w_0$ aleatorio y hacer iterativamente (una y otra vez) la siguiente actualización:\n",
    "\n",
    "$w_{t+1} = w_{t} - \\eta \\times f'(w_t)$\n",
    "\n",
    "Por ejemplo si queremos hallar el mínimo de la siguiente función cuadrática:\n",
    "\n",
    "$f(x) = (x-3)^2 + 10$\n",
    "\n",
    "Podemos hacerlo analíticamente: observar que es una parábola con vértice en $x=3$ y que este es el $x$ que arrojará valores mínimos. Particularmente:\n",
    "\n",
    "$f(3) = 10$\n",
    "\n",
    "\n",
    "Pero también podemos calcular el mínimo con el método de descenso por gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x-3)**2+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9700], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(1, requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda ejecuta un paso de descenso por gradiente. Si la ejecutamos sucesivas veces se puede observar que el valor de $f$ se reduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(0.9700387716293335) = 14.120742797851562\n"
     ]
    }
   ],
   "source": [
    "def iteration_gradient_descent(learning_rate = 0.01, show_value=False):\n",
    "    value = f(w)\n",
    "    if show_value:\n",
    "        print(f'f({w.item()}) = {value.item()}')\n",
    "    value.backward()\n",
    "    w.data = w.data - learning_rate * w.grad\n",
    "    w.grad.zero_()\n",
    "\n",
    "iteration_gradient_descent(show_value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando dentro de un loop para hacer muchas iteraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(1.0106379985809326) = 13.957561492919922\n",
      "f(2.7361714839935303) = 10.069605827331543\n",
      "f(2.96501088142395) = 10.001224517822266\n",
      "f(2.995359420776367) = 10.000021934509277\n",
      "f(2.999384880065918) = 10.0\n",
      "f(2.9999184608459473) = 10.0\n",
      "f(2.999988317489624) = 10.0\n",
      "f(2.9999942779541016) = 10.0\n",
      "f(2.9999942779541016) = 10.0\n",
      "f(2.9999942779541016) = 10.0\n"
     ]
    }
   ],
   "source": [
    "for iteration_number in range(1000):\n",
    "    iteration_gradient_descent(show_value = (iteration_number%100 == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente hemos encontrado el valor de $x$ que minimiza $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10.]), tensor([3.0000]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(w.data), w.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otra aplicación de descenso por gradiente\n",
    "\n",
    "Vimos que descenso por gradiente sirve para encontrar el mínimo de funciones. Pero también es útil en problemas que queremos encontrar cierta incógnita. Veremos cómo se puede hallar la inversa de una matriz con descenso por gradiente.\n",
    "\n",
    "Recordar que si tenemos una matriz $A$:\n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    0 & 1 & 4 \\\\\n",
    "    5 & 6 & 0\n",
    "\\end{bmatrix}\n",
    "Su inversa $A^{-1}$ es:\n",
    "\\begin{bmatrix}\n",
    "    -24 & 18 & 5 \\\\\n",
    "    20 & -15 & -4 \\\\\n",
    "    -5 & 4 & 1\n",
    "\\end{bmatrix}\n",
    "Debido a que el [producto](https://en.wikipedia.org/wiki/Matrix_multiplication) de ambas es la identidad $A \\times A^{-1} = I$. \n",
    "\n",
    "Recordar que la identidad $I$ es:\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 0 & 1\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [0., 1., 4.],\n",
       "        [5., 6., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [[1,2,3],\n",
    "          [0,1,4],\n",
    "          [5,6,0]]\n",
    "A = torch.tensor(values, dtype=torch.float)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen métodos para calcular la inversa de una matrix. Pytorch ya posee dicha funcionalidad como una función de librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-24.0000,  18.0000,   5.0000],\n",
       "        [ 20.0000, -15.0000,  -4.0000],\n",
       "        [ -5.0000,   4.0000,   1.0000]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed = torch.inverse(A)\n",
    "A_inversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch posee además métodos para definir matrices útiles como la identidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos corroborar que el producto de la inversa calculada por la matriz original es la identidad. Si se mira con atención en la diagonal hay todos $1$ y en las entradas que no pertenecen a la diagonal hay números muy pequeños (casi 0). Esto es producto a [errores numéricos](https://en.wikipedia.org/wiki/Numerical_error), (recordar que la representación como float no es exacta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  3.8147e-06,  0.0000e+00],\n",
       "        [ 0.0000e+00,  1.0000e+00, -3.8147e-06],\n",
       "        [ 4.7684e-07,  1.9073e-06,  1.0000e+00]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero cómo usamos descenso por gradiente para encontrar $A^{-1}$? No sirve sólo para problemas de minimización?\n",
    "\n",
    "$A^{-1}$ es la única matriz que multiplicada por $A$ da la identidad:\n",
    "\n",
    "$A \\times A^{-1} = I$\n",
    "\n",
    "Lo que es equivalente a decir:\n",
    "\n",
    "$A \\times A^{-1} - I = 0$\n",
    "\n",
    "Y a su vez, es equivalente a:\n",
    "\n",
    "$||A \\times A^{-1} - I ||_2 = 0$\n",
    "\n",
    "La norma $|| . ||_2$ es siempre positiva (o cero). Por lo cual, encontrar $A^{-1}$ es equivalente a minimizar:\n",
    "\n",
    "$f(W) = ||A \\times W - I ||_2 $\n",
    "\n",
    "Comenzaremos con una matriz $W_0$ aleatoria e iremos actualizando sus entradas con descenso por gradiente para minimizar $f$ definida tal como se muestra arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    dim = A.shape[0]\n",
    "    I = torch.eye(dim)\n",
    "    return torch.norm(A @ W - I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro $W$ inicial está lejos de ser la inversa $A_{-1}$ buscada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2712, 0.8705, 0.3353],\n",
       "        [0.1286, 0.1051, 0.8353],\n",
       "        [0.2796, 0.7981, 0.5629]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.rand((3,3), requires_grad=True)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(W)$ que puede ser interpretado como \"el error\" de nuestra solución está lejos de ser $0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.1950, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función que realiza una iteración de descenso por gradiente es igual que la que usamos para nuestra función cuadrática.\n",
    "También podemos corrobarar, una vez más que ejecutando sucesivas iteraciones el valor de $f$ es cada vez menor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(W) = 10.194985389709473\n",
      "tensor([[0.2712, 0.8705, 0.3353],\n",
      "        [0.1286, 0.1051, 0.8353],\n",
      "        [0.2796, 0.7981, 0.5629]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def iteration_gradient_descent(learning_rate = 0.01, show_value=False):\n",
    "    value = f(W)\n",
    "    if show_value:\n",
    "        print(f'f(W) = {value.item()}')\n",
    "        print(W)\n",
    "    value.backward()\n",
    "    W.data = W.data - learning_rate * W.grad\n",
    "    W.grad.zero_()\n",
    "\n",
    "iteration_gradient_descent(show_value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer varias iteraciones utilizaremos learning rates cada vez más chicos (recordar que al aproximarse al mínimo es conveniente reducir el learning rate para no \"saltar\" por encima del mínimo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(W) = 9.554241180419922\n",
      "tensor([[0.2604, 0.8426, 0.3037],\n",
      "        [0.1142, 0.0667, 0.7916],\n",
      "        [0.2736, 0.7789, 0.5400]], requires_grad=True)\n",
      "f(W) = 3.4269797801971436\n",
      "tensor([[-20.1295,  14.9954,   3.9967],\n",
      "        [ 16.6526, -12.8340,  -3.5485],\n",
      "        [ -4.1847,   3.3359,   0.7587]], requires_grad=True)\n",
      "f(W) = 3.4276392459869385\n",
      "tensor([[-23.4183,  17.3784,   4.6853],\n",
      "        [ 19.3907, -14.8181,  -4.1224],\n",
      "        [ -4.8851,   3.8434,   0.9053]], requires_grad=True)\n",
      "f(W) = 3.424090623855591\n",
      "tensor([[-23.9549,  17.7671,   4.7977],\n",
      "        [ 19.8374, -15.1414,  -4.2155],\n",
      "        [ -4.9994,   3.9262,   0.9293]], requires_grad=True)\n",
      "f(W) = 0.3360155522823334\n",
      "tensor([[-23.9694,  17.9594,   4.9746],\n",
      "        [ 19.9623, -14.9990,  -4.0166],\n",
      "        [ -4.9944,   3.9890,   0.9919]], requires_grad=True)\n",
      "f(W) = 0.3360155522823334\n",
      "tensor([[-23.9694,  17.9594,   4.9746],\n",
      "        [ 19.9623, -14.9990,  -4.0166],\n",
      "        [ -4.9944,   3.9890,   0.9919]], requires_grad=True)\n",
      "f(W) = 0.3360155522823334\n",
      "tensor([[-23.9694,  17.9594,   4.9746],\n",
      "        [ 19.9623, -14.9990,  -4.0166],\n",
      "        [ -4.9944,   3.9890,   0.9919]], requires_grad=True)\n",
      "f(W) = 0.03434545546770096\n",
      "tensor([[-23.9653,  17.9739,   4.9907],\n",
      "        [ 19.9698, -14.9816,  -3.9962],\n",
      "        [ -4.9927,   3.9942,   0.9977]], requires_grad=True)\n",
      "f(W) = 0.03434545546770096\n",
      "tensor([[-23.9653,  17.9739,   4.9907],\n",
      "        [ 19.9698, -14.9816,  -3.9962],\n",
      "        [ -4.9927,   3.9942,   0.9977]], requires_grad=True)\n",
      "f(W) = 0.03434545546770096\n",
      "tensor([[-23.9653,  17.9739,   4.9907],\n",
      "        [ 19.9698, -14.9816,  -3.9962],\n",
      "        [ -4.9927,   3.9942,   0.9977]], requires_grad=True)\n",
      "f(W) = 0.0034234588965773582\n",
      "tensor([[-23.9648,  17.9757,   4.9928],\n",
      "        [ 19.9708, -14.9795,  -3.9936],\n",
      "        [ -4.9925,   3.9949,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0034234588965773582\n",
      "tensor([[-23.9648,  17.9757,   4.9928],\n",
      "        [ 19.9708, -14.9795,  -3.9936],\n",
      "        [ -4.9925,   3.9949,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0034234588965773582\n",
      "tensor([[-23.9648,  17.9757,   4.9928],\n",
      "        [ 19.9708, -14.9795,  -3.9936],\n",
      "        [ -4.9925,   3.9949,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0014293285785242915\n",
      "tensor([[-23.9648,  17.9756,   4.9926],\n",
      "        [ 19.9707, -14.9797,  -3.9938],\n",
      "        [ -4.9925,   3.9948,   0.9984]], requires_grad=True)\n",
      "f(W) = 0.0014293285785242915\n",
      "tensor([[-23.9648,  17.9756,   4.9926],\n",
      "        [ 19.9707, -14.9797,  -3.9938],\n",
      "        [ -4.9925,   3.9948,   0.9984]], requires_grad=True)\n",
      "f(W) = 0.0014293285785242915\n",
      "tensor([[-23.9648,  17.9756,   4.9926],\n",
      "        [ 19.9707, -14.9797,  -3.9938],\n",
      "        [ -4.9925,   3.9948,   0.9984]], requires_grad=True)\n",
      "f(W) = 0.0014293285785242915\n",
      "tensor([[-23.9648,  17.9756,   4.9926],\n",
      "        [ 19.9707, -14.9797,  -3.9938],\n",
      "        [ -4.9925,   3.9948,   0.9984]], requires_grad=True)\n",
      "f(W) = 0.0014293285785242915\n",
      "tensor([[-23.9648,  17.9756,   4.9926],\n",
      "        [ 19.9707, -14.9797,  -3.9938],\n",
      "        [ -4.9925,   3.9948,   0.9984]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for lr_exponent in range(1,7):\n",
    "    n_iter_to_show = 100000\n",
    "    n_iter = n_iter_to_show * 3\n",
    "    learning_rate = 0.1 ** lr_exponent\n",
    "    for iteration_number in range(n_iter):\n",
    "        iteration_gradient_descent(show_value = (iteration_number%n_iter_to_show == 0), learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El $W$ obtenido es una aproximacion relativamente buena de $A^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-23.9648,  17.9756,   4.9926],\n",
       "        [ 19.9707, -14.9797,  -3.9938],\n",
       "        [ -4.9925,   3.9948,   0.9984]], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-24.0000,  18.0000,   5.0000],\n",
       "        [ 20.0000, -15.0000,  -4.0000],\n",
       "        [ -5.0000,   4.0000,   1.0000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3: \"Resolviendo un sistema de ecuaciones utilizando descenso por gradiente\" (opcional)\n",
    "\n",
    "Utilizar el método de descenso por gradiente para resolver el siguiente sistema de ecuaciones:\n",
    "\n",
    "$ 3 x + 4 y - 2 z = 0$\n",
    "\n",
    "$ 2 x - 3 y + 4 z = 11$\n",
    "\n",
    "$ x - 2 y + 3 z = 7$\n",
    "\n",
    "Hint: recordar la representación matricial del sistema de ecuaciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

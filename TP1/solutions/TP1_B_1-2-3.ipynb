{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e447b2",
   "metadata": {},
   "source": [
    "#### Ejercicio 1: \"Re-implementando\" (opcional):\n",
    "\n",
    "Copiar esta notebook y borrar las celdas donde creamos la red neuronal (desde la sección \"Creando nuestra red neuronal\" en adelante). De modo que sólo quede la parte donde importamos el dataset. Y re-escribir:\n",
    "- La creación de los pesos y la función que dado las features produce la predicción.\n",
    "- La función de costo (negative log likehood).\n",
    "- La métrica: accuracy.\n",
    "- El loop de entrenamiento, incluyendo código para evaluar en cada época y mostrar cómo mejora la métrica.\n",
    "- Un ejemplo de cómo se usa el modelo para inferencia (sobre un sólo caso, para calcular la predicción)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f63267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b7ab9f-8339-4eb2-a20b-62b2ae65dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = Path(\"../data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "# Creamos el directorio si no existe\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "# Descargamos mnist.pkl.gz utilizando un HTTP GET request\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3bcb5b-7c81-4ef4-988d-dcf4eda1b624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los arrays de las imagenes fueron guardados en un archivo formato pickle, que se utiliza para persistir variable en Python\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85dde03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para obtener un batch de imagenes\n",
    "def get_batch(a, i):\n",
    "    return torch.tensor(a[batch_size*i:batch_size*(i+1),...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe645af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de prediccion de la red\n",
    "def calculate_predictions(t):\n",
    "    linear_combination_1 = t @ W_1 + b_1\n",
    "    hidden_layer_1 = torch.max(torch.tensor(0), linear_combination_1)\n",
    "    \n",
    "    linear_combination_2 = hidden_layer_1 @ W_2 + b_2\n",
    "    hidden_layer_2 = torch.max(torch.tensor(0), linear_combination_2)\n",
    "\n",
    "    linear_combination_3 = hidden_layer_2 @ W_3 + b_3\n",
    "\n",
    "    return softmax(linear_combination_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cc9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para inicializar los pesos de la red\n",
    "def w_rand(n_in, n_out):\n",
    "    return torch.normal(0, math.sqrt(6) / math.sqrt(n_in + n_out), size=(n_in,n_out), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba92bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion softmax\n",
    "def softmax(x):\n",
    "    return x.exp() / x.exp().sum(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1fdcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de perdida logaritmica\n",
    "def nnl(pred_batch, target_batch):\n",
    "    bz = pred_batch.shape[0]\n",
    "    return -torch.log(pred_batch[range(bz), target_batch]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac17d4d-a1f3-497b-b419-f95c9ae2fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que calcula el accuracy de un batch de imagenes\n",
    "def accuracy(probs, target):\n",
    "    class_predictions = torch.argmax(probs, dim=1)\n",
    "    return (class_predictions == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccbe3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos los pesos de la red\n",
    "W_1 = w_rand(28*28, 512)\n",
    "b_1 = torch.zeros(512, requires_grad=True)\n",
    "\n",
    "W_2 = w_rand(512, 512)\n",
    "b_2 = torch.zeros(512, requires_grad=True)\n",
    "\n",
    "W_3 = w_rand(512,10)\n",
    "b_3 = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "weights = [W_1, b_1, W_2, b_2, W_3, b_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1fde06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "\n",
    "#calculo la cantidad de batches\n",
    "n_batches_train = x_train.shape[0] // batch_size\n",
    "n_batches_valid = (x_valid.shape[0] + batch_size - 1) // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b196dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train loss: 0.06399812686904578 - Validation loss: 0.09806575496181155 - Validation accuracy: 0.9725439548492432\n",
      "Epoch 1 - Train loss: 0.058486916950162116 - Validation loss: 0.10016746765274864 - Validation accuracy: 0.972244381904602\n",
      "Epoch 2 - Train loss: 0.05349240983343265 - Validation loss: 0.09314777689800707 - Validation accuracy: 0.9738418459892273\n",
      "Epoch 3 - Train loss: 0.04941959250633391 - Validation loss: 0.0931308099835629 - Validation accuracy: 0.9730431437492371\n",
      "Epoch 4 - Train loss: 0.045496526886303595 - Validation loss: 0.08948774709099958 - Validation accuracy: 0.9739416837692261\n",
      "Epoch 5 - Train loss: 0.04211268131218722 - Validation loss: 0.0871676202356137 - Validation accuracy: 0.9750399589538574\n",
      "Epoch 6 - Train loss: 0.03856687843252543 - Validation loss: 0.08874176073850641 - Validation accuracy: 0.9739416837692261\n",
      "Epoch 7 - Train loss: 0.03580513498520779 - Validation loss: 0.08789845608207716 - Validation accuracy: 0.9746405482292175\n",
      "Epoch 8 - Train loss: 0.03337051378759506 - Validation loss: 0.08591178959346571 - Validation accuracy: 0.975239634513855\n",
      "Epoch 9 - Train loss: 0.030900622084683162 - Validation loss: 0.08408256960595167 - Validation accuracy: 0.9755391478538513\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    lost_train_sum = 0\n",
    "\n",
    "    # Entrenamiento\n",
    "    for i in range(n_batches_train):\n",
    "        x_train_batch = get_batch(x_train, i)\n",
    "        y_train_batch = get_batch(y_train, i)\n",
    "    \n",
    "        # Calcular la prediccion del minibatch\n",
    "        preds = calculate_predictions(x_train_batch)\n",
    "\n",
    "        # Calcular la loss\n",
    "        loss = nnl(preds, y_train_batch)\n",
    "\n",
    "        # Actualizar los pesos\n",
    "        for w in weights:\n",
    "            if w.grad is not None:\n",
    "                w.grad.zero_()\n",
    "        \n",
    "        # Calcular el gradiente\n",
    "        loss.backward()\n",
    "\n",
    "        lost_train_sum += loss.item()\n",
    "\n",
    "        # actualizar el peso\n",
    "        with torch.no_grad():\n",
    "            for w in weights:\n",
    "                w -= w.grad * lr\n",
    "\n",
    "    # Volver a obtener un rango de batches random\n",
    "    permutation = torch.randperm(x_train.shape[0])\n",
    "\n",
    "    x_train = x_train[permutation,...]\n",
    "    y_train = y_train[permutation,...]\n",
    "\n",
    "    # Evaluamos los datos en validación\n",
    "    loss_validation_sum = 0\n",
    "    accuracy_sum = 0\n",
    "\n",
    "    for i in range(n_batches_valid):\n",
    "        x_batch = get_batch(x_valid, i)\n",
    "        y_batch = get_batch(y_valid, i)\n",
    "\n",
    "        # Calcular la prediccion del minibatch\n",
    "        preds = calculate_predictions(x_batch)\n",
    "\n",
    "        # Calcular la loss\n",
    "        loss = nnl(preds, y_batch)\n",
    "\n",
    "        loss_validation_sum += loss.item()\n",
    "\n",
    "        # Calcular el accuracy\n",
    "        accuracy_sum += accuracy(preds, y_batch)\n",
    "\n",
    "    print(f\"Epoch {epoch} - Train loss: {lost_train_sum/n_batches_train} - Validation loss: {loss_validation_sum/n_batches_valid} - Validation accuracy: {accuracy_sum/n_batches_valid}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6328222-cdcd-4eb2-b2ea-fc9c6366d9a9",
   "metadata": {},
   "source": [
    "#### Ejercicio 2: \"Analizando nuestro modelo\" (obligatorio)\n",
    "\n",
    "Usando como base alguno de los dos ejemplos realizar código que:\n",
    "- Calcule la matriz de confusión para los dígitos. La matriz de confusión es una métrica, por lo que debe ser calculada sólo en validación.\n",
    "- Utilizando la matriz de confusión elegir el par de dígitos dónde el modelo se confunde más y mostrar 20 ejemplos mal clasificados.\n",
    "- Una función que muestre los 20 ejemplos de validación con mayor costo. Intuitivamente estos son los ejemplos donde el modelo está más errado: arroja probabilidades altas para clases que no son la correcta.\n",
    "- Una función que muestre los 20 ejemplos de validación donde el modelo arroja probabilidades más bajas. Esto se puede interpretar como que el modelo está \"poco seguro\" para estos casos.\n",
    "\n",
    "Todo este trabajo de visualización suele ser comun realizarlo cuando se trabaja con datasets reales para limpiar el dataset de ejemplos malformados, maletiquetados, etc.\n",
    "En caso de utilizarlo para limpiar el dataset hay que realizarlo sobre todo el conjunto de datos (no sólo validación). Y es importante que el modelo no llegue a sobre-ajustar ni un poco (ya que usaremos por ejemplo el costo en training para limpiar el dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3db1b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creo un array para guardar los resultados\n",
    "results = []\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "confusion_matrix = torch.zeros((10, 10))\n",
    "\n",
    "# recorrer los batches de validación\n",
    "for i in range(n_batches_valid):\n",
    "    x_batch = get_batch(x_valid, i)\n",
    "    y_batch = get_batch(y_valid, i)\n",
    "\n",
    "    # Calcular las predicciones de los datos de validación\n",
    "    preds = calculate_predictions(x_batch)\n",
    "\n",
    "    # obtener la clase predicha\n",
    "    predicted_labels = torch.argmax(preds, dim=1)\n",
    "\n",
    "    #guardar los preds, los valores reales y las probabilidades\n",
    "    results.append((y_batch.numpy().tolist()\n",
    "                    , predicted_labels.numpy().tolist()\n",
    "                    , preds.detach().numpy().max(axis=1)\n",
    "    ))\n",
    "  \n",
    "    # Actualizar la matriz de confusión\n",
    "    for j in range(len(y_batch)):\n",
    "        confusion_matrix[y_batch[j], predicted_labels[j]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac8b5f",
   "metadata": {},
   "source": [
    "Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3455f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0       1      2      3      4      5      6       7      8      9\n",
      "0  974.0     0.0    8.0    0.0    0.0    1.0    3.0     0.0    2.0    3.0\n",
      "1    0.0  1056.0    2.0    2.0    0.0    0.0    1.0     1.0    2.0    0.0\n",
      "2    1.0     2.0  966.0    3.0    1.0    1.0    3.0     7.0    6.0    0.0\n",
      "3    0.0     0.0    6.0  999.0    0.0   13.0    0.0     2.0    6.0    4.0\n",
      "4    0.0     5.0    0.0    0.0  963.0    0.0    2.0     2.0    1.0   10.0\n",
      "5    4.0     1.0    2.0   17.0    3.0  864.0   12.0     1.0    7.0    4.0\n",
      "6    4.0     1.0    0.0    0.0    1.0    2.0  957.0     0.0    2.0    0.0\n",
      "7    1.0     2.0    4.0    2.0    2.0    0.0    0.0  1071.0    1.0    7.0\n",
      "8    0.0     4.0    4.0    8.0    0.0    4.0    1.0     1.0  981.0    6.0\n",
      "9    3.0     3.0    0.0    6.0    8.0    2.0    0.0    11.0    4.0  924.0\n"
     ]
    }
   ],
   "source": [
    "# Convertir a un pandas DataFrame\n",
    "confusion_df = pd.DataFrame(confusion_matrix.numpy(), columns=range(10), index=range(10))\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd48bf4",
   "metadata": {},
   "source": [
    "Los 2 digitos que más se confunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cab8375e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizando la matriz de confusión obtenemos los valores que más se confunden\n",
    "max_values = confusion_df.where(~np.eye(confusion_df.shape[0], dtype=bool)).max().max()\n",
    "\n",
    "# Obtengo los indices de los valores máximos\n",
    "indices = np.where(confusion_df == max_values)\n",
    "\n",
    "# Obtengo los valores de los indices\n",
    "digit1, digit2 = indices[0][0], indices[1][0]\n",
    "\n",
    "digit1, digit2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d893eb3",
   "metadata": {},
   "source": [
    "Los 20 primeros valores que son mal predichos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7681d35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>yhat</th>\n",
       "      <th>prob</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.547112</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.634591</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.511139</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.870687</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.547771</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.611102</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.532514</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.526388</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.577177</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.994454</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.904991</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.609259</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.983136</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519466</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.585730</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.679037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.944232</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.539557</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.372471</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.587360</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y  yhat      prob  correct\n",
       "91   5     4  0.547112    False\n",
       "192  0     6  0.634591    False\n",
       "212  3     9  0.511139    False\n",
       "239  9     0  0.870687    False\n",
       "246  8     7  0.547771    False\n",
       "317  3     8  0.611102    False\n",
       "318  9     7  0.532514    False\n",
       "320  5     7  0.526388    False\n",
       "322  5     3  0.577177    False\n",
       "329  9     7  0.994454    False\n",
       "340  3     2  0.904991    False\n",
       "355  6     1  0.609259    False\n",
       "369  3     9  0.983136    False\n",
       "371  7     0  0.519466    False\n",
       "390  0     9  0.585730    False\n",
       "391  9     4  0.679037    False\n",
       "403  3     9  0.944232    False\n",
       "414  9     3  0.539557    False\n",
       "426  7     8  0.372471    False\n",
       "428  6     0  0.587360    False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertor en un DataFrame\n",
    "df_results = pd.DataFrame(data=[(y, yhat, prob) for r in results for y, yhat, prob in zip(r[0], r[1], r[2])], columns=[\"y\", \"yhat\", \"prob\"])\n",
    "\n",
    "# Agregar columna de aciertos\n",
    "df_results[\"correct\"] = df_results[\"y\"] == df_results[\"yhat\"]\n",
    "\n",
    "# Filtar los 20 priemros no aciertos\n",
    "df_results[~df_results[\"correct\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8c217",
   "metadata": {},
   "source": [
    "Función que muestra los 20 ejemplos de validación con mayor costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a15354e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>yhat</th>\n",
       "      <th>prob</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999367</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999174</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7662</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.998715</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9731</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.996954</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.995817</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995762</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.994682</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.994454</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.991577</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.990477</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.988466</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.986647</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.983136</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4506</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.979997</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.979116</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.978753</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.978562</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.973855</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.966996</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y  yhat      prob  correct\n",
       "3507  5     8  0.999367    False\n",
       "9915  4     7  0.999174    False\n",
       "7662  5     6  0.999036    False\n",
       "1944  4     9  0.998715    False\n",
       "9731  5     6  0.996954    False\n",
       "9719  9     5  0.995817    False\n",
       "3216  9     0  0.995762    False\n",
       "1248  9     4  0.994682    False\n",
       "329   9     7  0.994454    False\n",
       "2241  8     9  0.991577    False\n",
       "2981  5     6  0.990477    False\n",
       "4097  7     9  0.988466    False\n",
       "3063  5     6  0.986647    False\n",
       "369   3     9  0.983136    False\n",
       "4506  1     2  0.979997    False\n",
       "2129  3     5  0.979116    False\n",
       "514   8     3  0.978753    False\n",
       "4074  3     5  0.978562    False\n",
       "4880  5     6  0.973855    False\n",
       "2324  9     7  0.966996    False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_loss(df, n=20):\n",
    "    return df.sort_values(\"prob\", ascending=False).head(n)\n",
    "\n",
    "top_loss(df_results[~df_results[\"correct\"]], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52756cb6",
   "metadata": {},
   "source": [
    "Casos donde el modelo arroja las probabilidades mas bajas sobre los aciertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "810611a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>yhat</th>\n",
       "      <th>prob</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.297857</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.341790</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6081</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.345887</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.375313</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.387597</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6586</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.387770</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.409243</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6939</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.419619</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.430644</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.431342</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7054</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.431828</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.453435</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3906</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.460910</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4913</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.464579</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475609</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.479355</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.487529</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.492310</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.497192</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.498554</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y  yhat      prob  correct\n",
       "120   1     1  0.297857     True\n",
       "2886  7     7  0.341790     True\n",
       "6081  1     1  0.345887     True\n",
       "2895  5     5  0.375313     True\n",
       "3978  5     5  0.387597     True\n",
       "6586  5     5  0.387770     True\n",
       "5294  4     4  0.409243     True\n",
       "6939  5     5  0.419619     True\n",
       "7718  7     7  0.430644     True\n",
       "6838  5     5  0.431342     True\n",
       "7054  8     8  0.431828     True\n",
       "7311  5     5  0.453435     True\n",
       "3906  8     8  0.460910     True\n",
       "4913  5     5  0.464579     True\n",
       "6717  1     1  0.475609     True\n",
       "236   0     0  0.479355     True\n",
       "5878  5     5  0.487529     True\n",
       "2140  4     4  0.492310     True\n",
       "4452  4     4  0.497192     True\n",
       "4926  8     8  0.498554     True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def menos_seguros(df, n=20):\n",
    "    return df.sort_values(\"prob\", ascending=True).head(n)\n",
    "\n",
    "menos_seguros(df_results[df_results[\"correct\"]], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87811a6f-5d7f-4947-8c7e-e93144b3a39d",
   "metadata": {},
   "source": [
    "#### Ejercicio 3: \"Fine-tunning\" (opcional)\n",
    "\n",
    "Re-escribir el código de la unidad anterior utilizando el modelo resnet34 con pesos entrenados para ImageNet.\n",
    "**Si deciden hacer este ejercicios las siguientes secciones les serán útiles**\n",
    "\n",
    "ResNet es una familia de arquitecturas de redes convolucionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3092baf-919c-4c80-88f9-8af04003a128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision, torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet_model = torchvision.models.resnet34(pretrained=True)\n",
    "resnet_model.fc = nn.Linear(512, 10)\n",
    "resnet_model.to(device)\n",
    "resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c553a0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 28, 28]) torch.Size([10000, 3, 28, 28])\n",
      "782 157\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convierto a tensores de 28x28\n",
    "train_new = torch.tensor(x_train.reshape(-1,28,28))\n",
    "valid_new = torch.tensor(x_valid.reshape(-1,28,28))\n",
    "\n",
    "# Apilo 3 veces para tener 3 canales\n",
    "train_new = torch.stack([train_new,train_new,train_new]).permute(1,0,2,3)\n",
    "valid_new = torch.stack([valid_new,valid_new,valid_new]).permute(1,0,2,3)\n",
    "\n",
    "# Verifico las dimensiones\n",
    "print(train_new.shape, valid_new.shape)\n",
    "\n",
    " # Armo los dataset con las imagenes y las etiquetas\n",
    "train_dataset_new  = TensorDataset(train_new, torch.tensor(y_train))\n",
    "valid_dataset_new  = TensorDataset((valid_new), torch.tensor(y_valid))\n",
    "\n",
    "# Creo los dataloaders\n",
    "train_dataloader_new = DataLoader(train_dataset_new, batch_size=64, shuffle=True)\n",
    "valid_dataloader_new = DataLoader(valid_dataset_new, batch_size=64, shuffle=True)\n",
    "\n",
    "# Verifico la cantidad de batches\n",
    "print(len(train_dataloader_new), len(valid_dataloader_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6aabb",
   "metadata": {},
   "source": [
    "Entrenar el modelo solo en la ultima capa lineal que fue modificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec3ac8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 1.754\n",
      "[1,   400] loss: 1.592\n",
      "[1,   600] loss: 1.677\n",
      "[2,   200] loss: 1.689\n",
      "[2,   400] loss: 1.705\n",
      "[2,   600] loss: 1.616\n",
      "[3,   200] loss: 1.690\n",
      "[3,   400] loss: 1.745\n",
      "[3,   600] loss: 1.723\n",
      "[4,   200] loss: 1.723\n",
      "[4,   400] loss: 1.700\n",
      "[4,   600] loss: 1.718\n",
      "[5,   200] loss: 1.740\n",
      "[5,   400] loss: 1.732\n",
      "[5,   600] loss: 1.841\n"
     ]
    }
   ],
   "source": [
    "# Establecer el atributo requires_grad de todos los parámetros en el modelo en False.\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Establecer el atributo requires_grad de los parámetros de la ultima capa en el modelo en True.\n",
    "for param in resnet_model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Definir funciones de perdida y optimizador\n",
    "loss_func_fc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Solo le paso los parametros de la ultima capa al optimizador\n",
    "optimizer = torch.optim.Adam(resnet_model.fc.parameters(), lr=0.01)\n",
    "\n",
    "# Entrenar el modelo en pocas epocas (3)\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader_new, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet_model(inputs)\n",
    "        loss = loss_func_fc(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "torch.save(resnet_model.state_dict(), \"resnet_model_new.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a498039e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet_model_fc_modified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cargar el modelo guardado\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m resnet_model_fc_modified\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet_model_fc_modified.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'resnet_model_fc_modified' is not defined"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo guardado\n",
    "resnet_model_fc_modified.load_state_dict(torch.load(\"resnet_model_fc_modified.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bbd11e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.6412478035059981 | validation loss 0.6130310084630289 | accuracy 0.3863817891373802\n",
      "epoch 1 | train loss 0.0475972165270283 | validation loss 0.225638683009988 | accuracy 0.4884185303514377\n",
      "epoch 2 | train loss 0.03259653347777203 | validation loss 0.08496644936920003 | accuracy 0.4917132587859425\n",
      "epoch 3 | train loss 0.030387666148762888 | validation loss 0.0741072650503098 | accuracy 0.4932108626198083\n",
      "epoch 4 | train loss 0.4455229650480679 | validation loss 0.05770228753882267 | accuracy 0.4397464057507987\n",
      "epoch 5 | train loss 0.028562948756372206 | validation loss 0.1412349658856845 | accuracy 0.4926617412140575\n",
      "epoch 6 | train loss 0.028841269691623508 | validation loss 0.055586384613658814 | accuracy 0.49291134185303515\n",
      "epoch 7 | train loss 0.028345280656436333 | validation loss 0.049346158851731256 | accuracy 0.49346046325878595\n",
      "epoch 8 | train loss 0.023960166927665092 | validation loss 0.0419473947392648 | accuracy 0.49460862619808305\n",
      "epoch 9 | train loss 0.03419317238530466 | validation loss 0.03989095334920114 | accuracy 0.49276158146964855\n"
     ]
    }
   ],
   "source": [
    "# Volver a establecer el atributo requires_grad de todos los parámetros en el modelo en True para entrenar el modelo completo.\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "n_epochs = 10\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet_model_fc_modified.parameters(), lr=0.01)\n",
    "\n",
    "for idx_epoch in range(n_epochs):\n",
    "    # Loop de entrenamiento\n",
    "    loss_train_sum = 0\n",
    "    n_batches_train = 0\n",
    "\n",
    "    for x_train_batch, y_train_batch in train_dataloader_new:\n",
    "        predictions = resnet_model(x_train_batch)\n",
    "        loss = loss_func(predictions, y_train_batch)\n",
    "        loss_train_sum += loss.item()\n",
    "        n_batches_train += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluamos los datos en validación\n",
    "    loss_validation_sum = 0\n",
    "    accuracy_sum = 0\n",
    "    for x_valid_batch, y_valid_batch in valid_dataloader_new:\n",
    "        predictions = resnet_model(x_valid_batch)\n",
    "        loss = loss_func(predictions, y_valid_batch)\n",
    "        loss_validation_sum += loss.item()\n",
    "        accuracy_sum += accuracy(predictions, y_valid_batch).item()\n",
    "    \n",
    "    # Imprimimos el loss en train y validación y la métrica (siempre en validación)\n",
    "    accuracy_validation = accuracy_sum / n_batches_valid\n",
    "    loss_validation = loss_validation_sum / n_batches_valid\n",
    "    train_validation = loss_train_sum / n_batches_train\n",
    "    print(f'epoch {idx_epoch} | train loss {loss_validation} | validation loss {train_validation} | accuracy {accuracy_validation}')\n",
    "\n",
    "    # Guardar el modelo entrenado\n",
    "torch.save(resnet_model.state_dict(), \"resnet_model_new.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c3df9-c5aa-4c3b-84f7-03dd58fed04a",
   "metadata": {},
   "source": [
    "Finalmente podemos calcular la predicción del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "de0d2540-8630-4352-88c0-8be5ee972a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10]), -16.87685775756836)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs = resnet_model(x_valid_batch)\n",
    "logprobs.shape, logprobs[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0dd0a-664d-4956-a372-98acd4f99d78",
   "metadata": {},
   "source": [
    "Esta última capa es completamente nueva. Los pesos son completamente aleatorios. Cuándo esto sucede lo recomendable es entrenar algunas épocas esta capa solamente y luego entrenar toda la red. Esto se puede lograr pasando al optimizer sólo los parámetros de ésta última capa. Y luego crear otro optimizer y pasarle todos los parámetros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb70a5-c29e-4aef-82a5-57a7526961ed",
   "metadata": {},
   "source": [
    "#### Ejercicio 4: \"Dataset propio\" (opcional)\n",
    "\n",
    "Crear un dataset de imágenes de tu interés y entrenar un clasificar ResNet (ver ejercicio anterior) en dicho dataset.\n",
    "\n",
    "Para crear un dataset se puede utilizar, por ejemplo la API de Bing para descargar automáticamente imágenes del resultado de una búsqueda: https://pub.aimind.so/build-your-dataset-with-bing-search-api-1adf6b550a3c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed556d23-e6f4-400f-8523-819a42916ca7",
   "metadata": {},
   "source": [
    "#### Ejercicio 5: \"Regresión sobre imágenes\" (opcional)\n",
    "\n",
    "Hacer un modelo basado en ResNet que prediga la posición de la cabeza dada una imágen.\n",
    "**Este es, probablemente, el ejercicio más difícil del TP.**\n",
    "Si alguien está dispuesto y comienza a hacerlo, le sugiero ir poniéndo sus avances en el foro para que lo pueda ir guiando.\n",
    "A grandes rasgos los pasos debieran ser:\n",
    "1. Implementar un dataset particular. E\n",
    "2. Las imágenes no tienen el mismo size. Para poder construir los batches, necesitamos que lo sean (todas las dimensiones deben ser iguales). Va a ser necesario aplicar recorte (cropp) y reescalamiento (resize). Estas transformaciones se suelen aplicar como transformaciones que aplica la clase Dataset implementada. Notar que los target (las coordenadas de la cabeza) cambian cuando se recorta una imágen. O sea que también hay que aplicar transformaciones sobre los target.\n",
    "3. El resto es similar al ejercicio anterior: hay que tomar un modelo ResNet ya entrenado (puede ser resnet34) y cambiarle la última capa para que calcule dos números: las dos coordenadas. Como ahora la salida no es una distribución de probabilidad no debemos aplicar softmax sobre la salida. Algo que podemos hacer es aplicar la función sigmoide a cada una de las salidas y escalarla, es una práctica habitual para problemas de regresión en un rango acotado.\n",
    "4. Como función de costo ahora tendremos MSE (que aplicaremos sobre cada coordenada)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
